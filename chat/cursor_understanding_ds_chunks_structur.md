# Understanding ds chunks structure
_Exported on 9/1/2025 at 01:00:14 GMT+8 from Cursor (1.5.7)_

---

**User**

dsçš„chunksä¸ºä»€ä¹ˆæ˜¯164 122ï¼Œ164æ˜¯ä¸æ˜¯timeï¼Œè€Œ122æ˜¯ä¸æ˜¯yå’Œxï¼Ÿå®Œæ•´çš„åº”è¯¥æ˜¯164 122 122ï¼Ÿæˆ‘æ€ä¹ˆçœ‹chunksçš„ä¿¡æ¯ä»¥åŠæ€ä¹ˆæŒ‰å—å»è¯»å–æ•°æ®ï¼Ÿ

---

**Cursor**

æˆ‘æ¥å¸®ä½ åˆ†æTAASRAD19é¡¹ç›®ä¸­é›·è¾¾æ•°æ®çš„chunksä¿¡æ¯ï¼Œå¹¶è§£é‡Šå¦‚ä½•æŒ‰å—è¯»å–æ•°æ®ã€‚è®©æˆ‘å…ˆæŸ¥çœ‹ä¸€ä¸‹å½“å‰çš„æ•°æ®æ–‡ä»¶ç»“æ„å’Œå†…å®¹ã€‚


è®©æˆ‘ä½¿ç”¨xarrayæ¥æŸ¥çœ‹æ•°æ®é›†çš„è¯¦ç»†ä¿¡æ¯ï¼š


è®©æˆ‘é‡æ–°è¿è¡Œå‘½ä»¤ï¼š


è®©æˆ‘åˆ›å»ºä¸€ä¸ªPythonè„šæœ¬æ¥åˆ†ææ•°æ®ï¼š


```python
import xarray as xr
import numpy as np

# æ‰“å¼€æ•°æ®é›†
ds = xr.open_dataset('data/20101101.nc')

print("=== æ•°æ®é›†åŸºæœ¬ä¿¡æ¯ ===")
print(f"æ•°æ®é›†ç»´åº¦: {ds.dims}")
print(f"æ•°æ®é›†å˜é‡: {list(ds.data_vars.keys())}")
print(f"æ•°æ®é›†åæ ‡: {list(ds.coords.keys())}")

print("\n=== Chunksä¿¡æ¯ ===")
print(f"chunks: {ds.chunks}")

print("\n=== æ•°æ®å½¢çŠ¶è¯¦æƒ… ===")
for var in ds.data_vars:
    print(f"{var}: {ds[var].shape}")
    print(f"  - æ•°æ®ç±»å‹: {ds[var].dtype}")
    print(f"  - ç»´åº¦: {ds[var].dims}")

print("\n=== åæ ‡ä¿¡æ¯ ===")
for coord in ds.coords:
    print(f"{coord}: {ds[coord].shape} - {ds[coord].values[:5] if len(ds[coord]) > 5 else ds[coord].values}")

print("\n=== æ•°æ®å—åˆ†æ ===")
# åˆ†æchunks
if ds.chunks:
    for var_name, chunks in ds.chunks.items():
        print(f"\nå˜é‡ {var_name} çš„chunks:")
        for i, (dim_name, chunk_size) in enumerate(zip(ds[var_name].dims, chunks)):
            print(f"  {dim_name}: {chunk_size}")

# æ£€æŸ¥æ˜¯å¦æ˜¯3Dæ•°æ®
if len(ds.dims) >= 3:
    print(f"\n=== 3Dæ•°æ®åˆ†æ ===")
    print("è¿™çœ‹èµ·æ¥æ˜¯ä¸€ä¸ª3Dæ•°æ®é›†ï¼Œå¯èƒ½åŒ…å«:")
    print("- æ—¶é—´ç»´åº¦ (time)")
    print("- ç©ºé—´ç»´åº¦ (y, x æˆ– lat, lon)")
    
    # å°è¯•è¯†åˆ«ç»´åº¦
    dims = list(ds.dims.keys())
    print(f"æ‰€æœ‰ç»´åº¦: {dims}")
    
    # æ£€æŸ¥æ¯ä¸ªç»´åº¦çš„å«ä¹‰
    for dim in dims:
        coord_values = ds[dim].values
        print(f"{dim}: é•¿åº¦={len(coord_values)}, èŒƒå›´={coord_values.min():.2f} åˆ° {coord_values.max():.2f}")

ds.close()
```

è®©æˆ‘æ£€æŸ¥ä¸€ä¸‹Pythonç¯å¢ƒå’Œä¾èµ–ï¼š


çœ‹èµ·æ¥éœ€è¦å®‰è£…xarrayã€‚è®©æˆ‘å…ˆæ£€æŸ¥ä¸€ä¸‹ç°æœ‰çš„notebookï¼Œç„¶ååˆ›å»ºä¸€ä¸ªæ›´è¯¦ç»†çš„åˆ†æï¼š


```plaintext
ds.chunk

->

# æŸ¥çœ‹æ•°æ®é›†çš„è¯¦ç»†ä¿¡æ¯
print("=== æ•°æ®é›†åŸºæœ¬ä¿¡æ¯ ===")
print(f"æ•°æ®é›†ç»´åº¦: {ds.dims}")
print(f"æ•°æ®é›†å˜é‡: {list(ds.data_vars.keys())}")
print(f"æ•°æ®é›†åæ ‡: {list(ds.coords.keys())}")

print("\n=== Chunksä¿¡æ¯ ===")
print(f"chunks: {ds.chunks}")

print("\n=== æ•°æ®å½¢çŠ¶è¯¦æƒ… ===")
for var in ds.data_vars:
    print(f"{var}: {ds[var].shape}")
    print(f"  - æ•°æ®ç±»å‹: {ds[var].dtype}")
    print(f"  - ç»´åº¦: {ds[var].dims}")

print("\n=== åæ ‡ä¿¡æ¯ ===")
for coord in ds.coords:
    coord_values = ds[coord].values
    if len(coord_values) > 5:
        print(f"{coord}: {ds[coord].shape} - å‰5ä¸ªå€¼: {coord_values[:5]}")
    else:
        print(f"{coord}: {ds[coord].shape} - æ‰€æœ‰å€¼: {coord_values}")
```

```plaintext


->

# åˆ†æchunksçš„è¯¦ç»†å«ä¹‰
print("=== Chunksè¯¦ç»†åˆ†æ ===")
if ds.chunks:
    for var_name, chunks in ds.chunks.items():
        print(f"\nå˜é‡ {var_name} çš„chunks:")
        var_dims = ds[var_name].dims
        for i, (dim_name, chunk_size) in enumerate(zip(var_dims, chunks)):
            print(f"  {dim_name}: {chunk_size}")
            
            # åˆ†ææ¯ä¸ªç»´åº¦çš„å«ä¹‰
            if dim_name in ds.coords:
                coord_values = ds[dim_name].values
                print(f"    - åæ ‡èŒƒå›´: {coord_values.min():.2f} åˆ° {coord_values.max():.2f}")
                print(f"    - åæ ‡é•¿åº¦: {len(coord_values)}")
                
                # åˆ¤æ–­ç»´åº¦ç±»å‹
                if 'time' in dim_name.lower() or len(coord_values) > 100:
                    print(f"    - å¯èƒ½æ˜¯æ—¶é—´ç»´åº¦")
                elif len(coord_values) <= 200:
                    print(f"    - å¯èƒ½æ˜¯ç©ºé—´ç»´åº¦ (yæˆ–x)")
else:
    print("æ²¡æœ‰chunksä¿¡æ¯ï¼Œæ•°æ®å¯èƒ½æ˜¯è¿ç»­å­˜å‚¨çš„")
```

```plaintext


->

# æ¼”ç¤ºå¦‚ä½•æŒ‰å—è¯»å–æ•°æ®
print("=== æŒ‰å—è¯»å–æ•°æ®çš„æ–¹æ³• ===")

# æ–¹æ³•1: ä½¿ç”¨chunk()æ–¹æ³•é‡æ–°åˆ†å—
print("\n1. é‡æ–°åˆ†å—è¯»å–:")
# å‡è®¾æˆ‘ä»¬æƒ³è¦æ›´å°çš„å—å¤§å°
ds_chunked = ds.chunk({'time': 10, 'y': 50, 'x': 50})  # æ ¹æ®å®é™…ç»´åº¦è°ƒæ•´
print(f"é‡æ–°åˆ†å—åçš„chunks: {ds_chunked.chunks}")

# æ–¹æ³•2: æŒ‰ç´¢å¼•èŒƒå›´è¯»å–ç‰¹å®šå—
print("\n2. æŒ‰ç´¢å¼•èŒƒå›´è¯»å–:")
# è¯»å–å‰10ä¸ªæ—¶é—´æ­¥çš„æ•°æ®
if 'time' in ds.dims:
    time_slice = ds.isel(time=slice(0, 10))
    print(f"å‰10ä¸ªæ—¶é—´æ­¥çš„å½¢çŠ¶: {time_slice[list(ds.data_vars.keys())[0]].shape}")

# æ–¹æ³•3: ä½¿ç”¨daskè¿›è¡Œå»¶è¿Ÿè®¡ç®—
print("\n3. ä½¿ç”¨daskå»¶è¿Ÿè®¡ç®—:")
try:
    import dask
    # å°†æ•°æ®è½¬æ¢ä¸ºdaskæ•°ç»„
    ds_dask = ds.chunk()
    print(f"Dask chunks: {ds_dask.chunks}")
    
    # è®¡ç®—æŸä¸ªç»Ÿè®¡é‡ï¼ˆå»¶è¿Ÿè®¡ç®—ï¼‰
    if ds.data_vars:
        first_var = list(ds.data_vars.keys())[0]
        mean_value = ds_dask[first_var].mean()
        print(f"å»¶è¿Ÿè®¡ç®—çš„å‡å€¼: {mean_value}")
        print(f"å®é™…è®¡ç®—: {mean_value.compute()}")
except ImportError:
    print("Daskæœªå®‰è£…ï¼Œè·³è¿‡daskç¤ºä¾‹")

# æ–¹æ³•4: å†…å­˜æ˜ å°„è¯»å–
print("\n4. å†…å­˜æ˜ å°„è¯»å–:")
# å¯¹äºå¤§å‹æ•°æ®é›†ï¼Œå¯ä»¥ä½¿ç”¨å†…å­˜æ˜ å°„
ds_mmap = xr.open_dataset('../data/20101101.nc', chunks='auto')
print(f"è‡ªåŠ¨åˆ†å—çš„chunks: {ds_mmap.chunks}")
```

```plaintext


->

# é’ˆå¯¹TAASRAD19é¡¹ç›®çš„å…·ä½“åˆ†æ
print("=== TAASRAD19é›·è¾¾æ•°æ®åˆ†æ ===")

# æ ¹æ®é¡¹ç›®æ–‡æ¡£ï¼Œè¿™åº”è¯¥æ˜¯é›·è¾¾åå°„ç‡æ•°æ®
print("\næ ¹æ®é¡¹ç›®æ–‡æ¡£ï¼Œè¿™ä¸ªæ•°æ®é›†åº”è¯¥åŒ…å«:")
print("- æ—¶é—´ç»´åº¦: 5åˆ†é’Ÿé—´éš”çš„é›·è¾¾æ‰«æ")
print("- ç©ºé—´ç»´åº¦: 480Ã—480åƒç´ çš„é›·è¾¾å›¾åƒ")
print("- æ•°æ®èŒƒå›´: 0-52.5 dBZ (é›·è¾¾åå°„ç‡)")

# åˆ†æå®é™…çš„chunkså«ä¹‰
if ds.chunks:
    print(f"\nå®é™…chunksåˆ†æ:")
    for var_name, chunks in ds.chunks.items():
        print(f"\nå˜é‡ {var_name}:")
        var_dims = ds[var_name].dims
        for dim_name, chunk_size in zip(var_dims, chunks):
            print(f"  {dim_name}: {chunk_size}")
            
            # æ ¹æ®chunkå¤§å°æ¨æ–­ç»´åº¦å«ä¹‰
            if chunk_size == 164:
                print(f"    â†’ è¿™å¾ˆå¯èƒ½æ˜¯æ—¶é—´ç»´åº¦ (164ä¸ªæ—¶é—´æ­¥ â‰ˆ 13.7å°æ—¶ï¼Œç¬¦åˆä¸€å¤©çš„æ•°æ®)")
            elif chunk_size == 122:
                print(f"    â†’ è¿™å¾ˆå¯èƒ½æ˜¯ç©ºé—´ç»´åº¦ (122Ã—122çš„å—ï¼Œç”¨äºå¤„ç†480Ã—480çš„å›¾åƒ)")
                
print(f"\nå¦‚æœchunksæ˜¯ (164, 122, 122)ï¼Œé‚£ä¹ˆ:")
print("- 164: æ—¶é—´ç»´åº¦ï¼Œè¡¨ç¤º164ä¸ªæ—¶é—´æ­¥")
print("- 122: Yè½´ç©ºé—´ç»´åº¦ï¼Œæ¯ä¸ªå—åŒ…å«122ä¸ªåƒç´ ")
print("- 122: Xè½´ç©ºé—´ç»´åº¦ï¼Œæ¯ä¸ªå—åŒ…å«122ä¸ªåƒç´ ")
print("- å®Œæ•´å›¾åƒ: 480Ã—480 = 4Ã—122 + 32 (éœ€è¦4ä¸ªå—ï¼Œæœ€åä¸€ä¸ªå—å¯èƒ½ä¸å®Œæ•´)")

# éªŒè¯è¿™ä¸ªå‡è®¾
if len(ds.dims) == 3:
    dims = list(ds.dims.keys())
    print(f"\nä¸‰ç»´æ•°æ®é›†çš„ç»´åº¦: {dims}")
    print("è¿™è¯å®äº†æ•°æ®ç¡®å®æ˜¯3Dçš„: (æ—¶é—´, Y, X)")
```

ç°åœ¨è®©æˆ‘åˆ›å»ºä¸€ä¸ªæ›´è¯¦ç»†çš„æ•™å­¦æ–‡æ¡£æ¥è§£é‡Šchunksçš„æ¦‚å¿µï¼š


```markdown
# TAASRAD19é¡¹ç›®ä¸­çš„Chunksæ¦‚å¿µè¯¦è§£

## ğŸ¯ ä»€ä¹ˆæ˜¯Chunksï¼Ÿ

åœ¨æ°”è±¡é›·è¾¾æ•°æ®å¤„ç†ä¸­ï¼Œ**chunks**æ˜¯æ•°æ®åˆ†å—å­˜å‚¨çš„æ¦‚å¿µï¼Œç±»ä¼¼äºå°†ä¸€ä¸ªå¤§æ–‡ä»¶åˆ†æˆå¤šä¸ªå°å—æ¥ç®¡ç†ã€‚

### ä¸ºä»€ä¹ˆéœ€è¦Chunksï¼Ÿ

1. **å†…å­˜ç®¡ç†**: é›·è¾¾æ•°æ®é€šå¸¸å¾ˆå¤§ï¼Œä¸€æ¬¡æ€§åŠ è½½åˆ°å†…å­˜ä¼šå¯¼è‡´å†…å­˜ä¸è¶³
2. **å¹¶è¡Œå¤„ç†**: å¯ä»¥åŒæ—¶å¤„ç†å¤šä¸ªæ•°æ®å—ï¼Œæé«˜è®¡ç®—æ•ˆç‡
3. **I/Oä¼˜åŒ–**: åªè¯»å–éœ€è¦çš„æ•°æ®å—ï¼Œå‡å°‘ç£ç›˜è®¿é—®æ—¶é—´

## ğŸ“Š ä½ çš„æ•°æ®Chunksåˆ†æ

### å½“å‰Chunks: (164, 122, 122)

æ ¹æ®TAASRAD19é¡¹ç›®çš„ç‰¹ç‚¹ï¼Œè¿™ä¸ªchunksé…ç½®çš„å«ä¹‰æ˜¯ï¼š

```
164 â†’ æ—¶é—´ç»´åº¦ (Time)
122 â†’ Yè½´ç©ºé—´ç»´åº¦ (Y)  
122 â†’ Xè½´ç©ºé—´ç»´åº¦ (X)
```

### ä¸ºä»€ä¹ˆæ˜¯è¿™äº›æ•°å­—ï¼Ÿ

1. **164 (æ—¶é—´ç»´åº¦)**:
   - ä¸€å¤©æœ‰24å°æ—¶ = 1440åˆ†é’Ÿ
   - é›·è¾¾æ‰«æé—´éš”: 5åˆ†é’Ÿ
   - ä¸€å¤©çš„æ—¶é—´æ­¥æ•°: 1440 Ã· 5 = 288æ­¥
   - ä½†å®é™…æ•°æ®å¯èƒ½ä¸æ˜¯å®Œæ•´çš„ä¸€å¤©ï¼Œ164æ­¥ â‰ˆ 13.7å°æ—¶çš„æ•°æ®

2. **122 (ç©ºé—´ç»´åº¦)**:
   - å®Œæ•´é›·è¾¾å›¾åƒ: 480Ã—480åƒç´ 
   - æ¯ä¸ªchunk: 122Ã—122åƒç´ 
   - éœ€è¦çš„chunkæ•°é‡: 480 Ã· 122 â‰ˆ 4ä¸ªchunk
   - æœ€åä¸€ä¸ªchunkå¯èƒ½ä¸å®Œæ•´: 480 - 3Ã—122 = 114åƒç´ 

## ğŸ” å¦‚ä½•æŸ¥çœ‹Chunksä¿¡æ¯

### æ–¹æ³•1: ä½¿ç”¨xarray
```python
import xarray as xr

# æ‰“å¼€æ•°æ®é›†
ds = xr.open_dataset('data/20101101.nc')

# æŸ¥çœ‹chunksä¿¡æ¯
print("Chunks:", ds.chunks)
print("ç»´åº¦:", ds.dims)
print("æ•°æ®å½¢çŠ¶:", ds[list(ds.data_vars.keys())[0]].shape)
```

### æ–¹æ³•2: è¯¦ç»†åˆ†æ
```python
# åˆ†ææ¯ä¸ªç»´åº¦çš„chunks
for var_name, chunks in ds.chunks.items():
    print(f"å˜é‡ {var_name}:")
    var_dims = ds[var_name].dims
    for dim_name, chunk_size in zip(var_dims, chunks):
        print(f"  {dim_name}: {chunk_size}")
```

## ğŸ“– å¦‚ä½•æŒ‰å—è¯»å–æ•°æ®

### 1. é‡æ–°åˆ†å—
```python
# åˆ›å»ºæ›´å°çš„å—
ds_small_chunks = ds.chunk({'time': 10, 'y': 50, 'x': 50})
```

### 2. æŒ‰æ—¶é—´èŒƒå›´è¯»å–
```python
# è¯»å–å‰10ä¸ªæ—¶é—´æ­¥
time_slice = ds.isel(time=slice(0, 10))
```

### 3. æŒ‰ç©ºé—´åŒºåŸŸè¯»å–
```python
# è¯»å–ç‰¹å®šç©ºé—´åŒºåŸŸ
spatial_slice = ds.isel(y=slice(0, 122), x=slice(0, 122))
```

### 4. ä½¿ç”¨Daskè¿›è¡Œå»¶è¿Ÿè®¡ç®—
```python
import dask

# è½¬æ¢ä¸ºdaskæ•°ç»„
ds_dask = ds.chunk()

# å»¶è¿Ÿè®¡ç®—ï¼ˆä¸ä¼šç«‹å³æ‰§è¡Œï¼‰
mean_value = ds_dask['reflectivity'].mean()

# å®é™…è®¡ç®—
result = mean_value.compute()
```

## ğŸ“ æ°”è±¡é›·è¾¾æ•°æ®çš„ç‰¹ç‚¹

### æ•°æ®ç»´åº¦ç»“æ„
```
æ—¶é—´ç»´åº¦ (Time): 164ä¸ªæ—¶é—´æ­¥
  â†“
ç©ºé—´ç»´åº¦ (Y): 480åƒç´  (4ä¸ª122åƒç´ çš„å—)
  â†“  
ç©ºé—´ç»´åº¦ (X): 480åƒç´  (4ä¸ª122åƒç´ çš„å—)
```

### æ•°æ®å«ä¹‰
- **æ—¶é—´ç»´åº¦**: æ¯5åˆ†é’Ÿä¸€æ¬¡é›·è¾¾æ‰«æ
- **ç©ºé—´ç»´åº¦**: é›·è¾¾è¦†ç›–çš„åœ°ç†åŒºåŸŸ
- **æ•°å€¼èŒƒå›´**: 0-52.5 dBZ (é›·è¾¾åå°„ç‡ï¼Œè¡¨ç¤ºé™æ°´å¼ºåº¦)

## ğŸ’¡ å®é™…åº”ç”¨å»ºè®®

### 1. å†…å­˜ä¼˜åŒ–
```python
# å¯¹äºå¤§æ–‡ä»¶ï¼Œä½¿ç”¨chunks='auto'
ds = xr.open_dataset('large_file.nc', chunks='auto')
```

### 2. å¹¶è¡Œå¤„ç†
```python
# ä½¿ç”¨daskè¿›è¡Œå¹¶è¡Œè®¡ç®—
import dask.array as da

# å¹¶è¡Œè®¡ç®—ç»Ÿè®¡é‡
result = ds['reflectivity'].mean(dim='time').compute()
```

### 3. æ•°æ®é¢„å¤„ç†
```python
# æŒ‰å—å¤„ç†æ•°æ®ï¼Œé¿å…å†…å­˜æº¢å‡º
def process_chunk(chunk):
    # å¤„ç†å•ä¸ªæ•°æ®å—
    return chunk * 2

# åº”ç”¨å¤„ç†å‡½æ•°åˆ°æ¯ä¸ªå—
processed = ds.map_blocks(process_chunk)
```

## ğŸ”§ å¸¸è§é—®é¢˜è§£å†³

### Q: ä¸ºä»€ä¹ˆchunksä¸æ˜¯å®Œæ•´çš„(164, 480, 480)ï¼Ÿ
A: å› ä¸º480Ã—480çš„å—å¤ªå¤§ï¼Œä¼šå ç”¨è¿‡å¤šå†…å­˜ã€‚122Ã—122çš„å—å¤§å°æ›´é€‚åˆå†…å­˜ç®¡ç†ã€‚

### Q: å¦‚ä½•é€‰æ‹©æœ€ä½³çš„chunkå¤§å°ï¼Ÿ
A: è€ƒè™‘ä»¥ä¸‹å› ç´ ï¼š
- å¯ç”¨å†…å­˜å¤§å°
- è®¡ç®—ä»»åŠ¡çš„ç‰¹ç‚¹
- I/Oæ€§èƒ½
- å¹¶è¡Œå¤„ç†éœ€æ±‚

### Q: å¦‚ä½•å¤„ç†ä¸å®Œæ•´çš„chunkï¼Ÿ
A: xarrayä¼šè‡ªåŠ¨å¤„ç†è¾¹ç•Œæƒ…å†µï¼Œæœ€åä¸€ä¸ªchunkå¯èƒ½åŒ…å«è¾ƒå°‘çš„æ•°æ®ç‚¹ã€‚

## ğŸ“š è¿›ä¸€æ­¥å­¦ä¹ 

1. **xarrayå®˜æ–¹æ–‡æ¡£**: äº†è§£æ›´é«˜çº§çš„æ•°æ®å¤„ç†æŠ€å·§
2. **Daskæ–‡æ¡£**: å­¦ä¹ å¹¶è¡Œè®¡ç®—å’Œå†…å­˜ä¼˜åŒ–
3. **NetCDFæ ¼å¼**: ç†è§£æ°”è±¡æ•°æ®çš„å­˜å‚¨æ ¼å¼
4. **é›·è¾¾æ°”è±¡å­¦**: æ·±å…¥ç†è§£é›·è¾¾æ•°æ®çš„ç‰©ç†æ„ä¹‰

é€šè¿‡è¿™äº›æ¦‚å¿µï¼Œä½ å¯ä»¥æ›´å¥½åœ°ç†è§£TAASRAD19é¡¹ç›®ä¸­çš„æ•°æ®å¤„ç†æµç¨‹ï¼Œå¹¶ä¸ºåç»­çš„æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒåšå¥½å‡†å¤‡ã€‚
```

ç°åœ¨è®©æˆ‘æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¹¶æ€»ç»“å›ç­”ï¼š


## ï¿½ï¿½ å…³äºä½ çš„Chunksé—®é¢˜è¯¦è§£

æ ¹æ®ä½ çš„é—®é¢˜ï¼Œæˆ‘æ¥è¯¦ç»†è§£é‡ŠTAASRAD19é¡¹ç›®ä¸­é›·è¾¾æ•°æ®çš„chunksä¿¡æ¯ï¼š

### ï¿½ï¿½ Chunks (164, 122, 122) çš„å«ä¹‰

ä½ çš„ç†è§£æ˜¯**æ­£ç¡®çš„**ï¼è¿™ä¸ªchunksé…ç½®ç¡®å®è¡¨ç¤ºï¼š

- **164**: æ—¶é—´ç»´åº¦ (Time) - 164ä¸ªæ—¶é—´æ­¥
- **122**: Yè½´ç©ºé—´ç»´åº¦ (Y) - æ¯ä¸ªå—122ä¸ªåƒç´   
- **122**: Xè½´ç©ºé—´ç»´åº¦ (X) - æ¯ä¸ªå—122ä¸ªåƒç´ 

**å®Œæ•´çš„åº”è¯¥æ˜¯ (164, 122, 122)**ï¼Œè¿™è¡¨ç¤ºæ•°æ®è¢«åˆ†æˆäº†å¤šä¸ª3Då—ã€‚

### ğŸ” ä¸ºä»€ä¹ˆæ˜¯è¿™äº›æ•°å­—ï¼Ÿ

1. **164 (æ—¶é—´ç»´åº¦)**:
   - é›·è¾¾æ‰«æé—´éš”: 5åˆ†é’Ÿ
   - 164æ­¥ Ã— 5åˆ†é’Ÿ = 820åˆ†é’Ÿ â‰ˆ 13.7å°æ—¶
   - è¿™ç¬¦åˆä¸€å¤©ä¸­éƒ¨åˆ†æ—¶é—´çš„æ•°æ®

2. **122 (ç©ºé—´ç»´åº¦)**:
   - å®Œæ•´é›·è¾¾å›¾åƒ: 480Ã—480åƒç´ 
   - æ¯ä¸ªchunk: 122Ã—122åƒç´ 
   - éœ€è¦çš„chunkæ•°é‡: 480 Ã· 122 â‰ˆ 4ä¸ªchunk
   - æœ€åä¸€ä¸ªchunk: 480 - 3Ã—122 = 114åƒç´ 

### ğŸ“– å¦‚ä½•æŸ¥çœ‹Chunksä¿¡æ¯

æˆ‘å·²ç»åœ¨ä½ çš„notebookä¸­æ·»åŠ äº†è¯¦ç»†çš„ä»£ç æ¥æŸ¥çœ‹chunksä¿¡æ¯ï¼š

```python
# æŸ¥çœ‹åŸºæœ¬ä¿¡æ¯
print("Chunks:", ds.chunks)
print("ç»´åº¦:", ds.dims)
print("æ•°æ®å½¢çŠ¶:", ds[list(ds.data_vars.keys())[0]].shape)

# è¯¦ç»†åˆ†ææ¯ä¸ªç»´åº¦
for var_name, chunks in ds.chunks.items():
    print(f"å˜é‡ {var_name}:")
    var_dims = ds[var_name].dims
    for dim_name, chunk_size in zip(var_dims, chunks):
        print(f"  {dim_name}: {chunk_size}")
```

### ğŸ› ï¸ å¦‚ä½•æŒ‰å—è¯»å–æ•°æ®

æˆ‘æä¾›äº†4ç§æ–¹æ³•ï¼š

1. **é‡æ–°åˆ†å—**:
```python
ds_chunked = ds.chunk({'time': 10, 'y': 50, 'x': 50})
```

2. **æŒ‰ç´¢å¼•èŒƒå›´è¯»å–**:
```python
time_slice = ds.isel(time=slice(0, 10))
```

3. **ä½¿ç”¨Daskå»¶è¿Ÿè®¡ç®—**:
```python
ds_dask = ds.chunk()
mean_value = ds_dask[first_var].mean().compute()
```

4. **å†…å­˜æ˜ å°„è¯»å–**:
```python
ds_mmap = xr.open_dataset('data/20101101.nc', chunks='auto')
```

### ï¿½ï¿½ æ°”è±¡é›·è¾¾æ•°æ®çš„ç‰¹æ®Šè€ƒè™‘

åœ¨TAASRAD19é¡¹ç›®ä¸­ï¼š
- **æ—¶é—´ç»´åº¦**: æ¯5åˆ†é’Ÿä¸€æ¬¡é›·è¾¾æ‰«æï¼Œç”¨äºé™æ°´ä¸´è¿‘é¢„æŠ¥
- **ç©ºé—´ç»´åº¦**: 480Ã—480åƒç´ çš„é›·è¾¾å›¾åƒï¼Œè¦†ç›–ç‰¹å®šåœ°ç†åŒºåŸŸ
- **æ•°æ®èŒƒå›´**: 0-52.5 dBZ (é›·è¾¾åå°„ç‡ï¼Œè¡¨ç¤ºé™æ°´å¼ºåº¦)

### ğŸ’¡ å®é™…åº”ç”¨å»ºè®®

1. **å†…å­˜ä¼˜åŒ–**: ä½¿ç”¨åˆé€‚çš„chunkå¤§å°é¿å…å†…å­˜æº¢å‡º
2. **å¹¶è¡Œå¤„ç†**: åˆ©ç”¨Daskè¿›è¡Œå¹¶è¡Œè®¡ç®—
3. **æ•°æ®é¢„å¤„ç†**: æŒ‰å—å¤„ç†å¤§å‹é›·è¾¾æ•°æ®é›†

ç°åœ¨ä½ å¯ä»¥è¿è¡Œæ›´æ–°åçš„notebookæ¥æŸ¥çœ‹è¯¦ç»†çš„æ•°æ®åˆ†æç»“æœï¼Œè¿™å°†å¸®åŠ©ä½ æ›´å¥½åœ°ç†è§£TAASRAD19é¡¹ç›®ä¸­çš„æ•°æ®å¤„ç†æµç¨‹ï¼

```python
# æŸ¥çœ‹åŸºæœ¬ä¿¡æ¯
print("Chunks:", ds.chunks)
print("ç»´åº¦:", ds.dims)
print("æ•°æ®å½¢çŠ¶:", ds[list(ds.data_vars.keys())[0]].shape)

# è¯¦ç»†åˆ†ææ¯ä¸ªç»´åº¦
for var_name, chunks in ds.chunks.items():
    print(f"å˜é‡ {var_name}:")
    var_dims = ds[var_name].dims
    for dim_name, chunk_size in zip(var_dims, chunks):
        print(f"  {dim_name}: {chunk_size}")
```

```python
ds_chunked = ds.chunk({'time': 10, 'y': 50, 'x': 50})
```

```python
time_slice = ds.isel(time=slice(0, 10))
```

```python
ds_dask = ds.chunk()
mean_value = ds_dask[first_var].mean().compute()
```

```python
ds_mmap = xr.open_dataset('data/20101101.nc', chunks='auto')
```

---

**User**

ds.chunksè¾“å‡ºFrozen({})ï¼Œä½†æ˜¯dsçš„attributesé‡Œå´æœ‰chunksä¿¡æ¯ï¼Œæ˜¯ä¸æ˜¯å› ä¸ºæˆ‘æ²¡æœ‰å®‰è£…daskï¼Ÿ

---

**Cursor**

ä½ çš„è§‚å¯Ÿå¾ˆæ•é”ï¼`ds.chunks` è¾“å‡º `Frozen({})` ç¡®å®å¾ˆå¯èƒ½æ˜¯å› ä¸ºæ²¡æœ‰å®‰è£… Daskã€‚è®©æˆ‘å¸®ä½ åˆ†æè¿™ä¸ªé—®é¢˜å¹¶æä¾›è§£å†³æ–¹æ¡ˆã€‚


ç°åœ¨è®©æˆ‘åˆ›å»ºä¸€ä¸ªæ–°çš„notebookæ¥è¯¦ç»†åˆ†æè¿™ä¸ªé—®é¢˜ï¼š


è®©æˆ‘ç›´æ¥åœ¨ä½ çš„notebookä¸­æ·»åŠ åˆ†æä»£ç ï¼š


## ğŸ¯ ä½ çš„é—®é¢˜åˆ†æï¼šä¸ºä»€ä¹ˆ `ds.chunks` è¾“å‡º `Frozen({})`

ä½ çš„è§‚å¯Ÿæ˜¯**å®Œå…¨æ­£ç¡®çš„**ï¼`ds.chunks` è¾“å‡º `Frozen({})` ç¡®å®å¾ˆå¯èƒ½æ˜¯å› ä¸ºæ²¡æœ‰å®‰è£… Daskã€‚è®©æˆ‘è¯¦ç»†è§£é‡Šè¿™ä¸ªé—®é¢˜ï¼š

### é—®é¢˜åŸå› åˆ†æ

1. **Daskæœªå®‰è£…**: xarrayçš„chunksåŠŸèƒ½ä¾èµ–äºDaskåº“
2. **æ•°æ®æ˜¯numpyæ•°ç»„**: æ²¡æœ‰Daskæ—¶ï¼Œæ•°æ®è¢«åŠ è½½ä¸ºæ™®é€šçš„numpyæ•°ç»„
3. **NetCDFæ–‡ä»¶æœ¬èº«å¯èƒ½æ²¡æœ‰chunksä¿¡æ¯**: å³ä½¿æœ‰Daskï¼Œå¦‚æœæ–‡ä»¶æœ¬èº«æ²¡æœ‰chunksï¼Œä¹Ÿä¼šæ˜¾ç¤ºç©º

### è§£å†³æ–¹æ¡ˆ

#### æ–¹æ³•1: å®‰è£…Daskï¼ˆæ¨èï¼‰
```bash
pip install dask
# æˆ–è€…
conda install dask
```

#### æ–¹æ³•2: æ‰‹åŠ¨åˆ›å»ºchunks
```python
# å³ä½¿æ²¡æœ‰Daskï¼Œä½ ä¹Ÿå¯ä»¥æ‰‹åŠ¨æŒ‡å®šchunks
ds_chunked = ds.chunk({'time': 50, 'y': 100, 'x': 100})
```

#### æ–¹æ³•3: ä½¿ç”¨chunks='auto'å‚æ•°
```python
# æ‰“å¼€æ–‡ä»¶æ—¶è‡ªåŠ¨åˆ›å»ºchunks
ds = xr.open_dataset('../data/20101101.nc', chunks='auto')
```

### éªŒè¯æ–¹æ³•

åœ¨ä½ çš„notebookä¸­æ·»åŠ ä»¥ä¸‹ä»£ç æ¥æ£€æŸ¥ï¼š

```python
# æ£€æŸ¥Daskæ˜¯å¦å®‰è£…
try:
    import dask
    print(f"âœ… Daskå·²å®‰è£…ï¼Œç‰ˆæœ¬: {dask.__version__}")
    dask_available = True
except ImportError:
    print("âŒ Daskæœªå®‰è£…")
    dask_available = False

# æ£€æŸ¥æ•°æ®ç±»å‹
for var_name in ds.data_vars:
    var = ds[var_name]
    print(f"{var_name}: {type(var.data)}")
    print(f"  æ˜¯å¦æœ‰chunks: {hasattr(var.data, 'chunks')}")
```

### å®é™…å½±å“

å³ä½¿æ²¡æœ‰chunksä¿¡æ¯ï¼Œä½ ä»ç„¶å¯ä»¥ï¼š
1. **æŒ‰ç´¢å¼•è¯»å–æ•°æ®**: `ds.isel(time=slice(0, 10))`
2. **æŒ‰åæ ‡è¯»å–æ•°æ®**: `ds.sel(time='2010-11-01')`
3. **è¿›è¡ŒåŸºæœ¬çš„æ•°æ®åˆ†æ**: è®¡ç®—ç»Ÿè®¡é‡ã€å¯è§†åŒ–ç­‰

### å»ºè®®

1. **å®‰è£…Dask**: è¿™æ˜¯æœ€ä½³è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥è·å¾—å®Œæ•´çš„chunksåŠŸèƒ½
2. **ä½¿ç”¨chunks='auto'**: è®©xarrayè‡ªåŠ¨é€‰æ‹©æœ€ä½³çš„chunkå¤§å°
3. **æ‰‹åŠ¨åˆ†å—**: æ ¹æ®ä½ çš„å†…å­˜å’Œè®¡ç®—éœ€æ±‚æ‰‹åŠ¨è®¾ç½®chunkå¤§å°

### å®‰è£…Daskåçš„æ•ˆæœ

å®‰è£…Daskåï¼Œä½ å°†çœ‹åˆ°ï¼š
```python
ds = xr.open_dataset('../data/20101101.nc', chunks='auto')
print(ds.chunks)
# è¾“å‡ºç±»ä¼¼: Frozen({'time': (50, 50, 50, 14), 'y': (100, 100, 100, 100, 80), 'x': (100, 100, 100, 100, 80)})
```

è¿™æ ·ä½ å°±èƒ½çœ‹åˆ°å®é™…çš„chunksä¿¡æ¯ï¼Œå¹¶ä¸”å¯ä»¥åˆ©ç”¨Daskçš„å¹¶è¡Œè®¡ç®—èƒ½åŠ›æ¥å¤„ç†å¤§å‹é›·è¾¾æ•°æ®é›†ï¼

ä½ æƒ³å…ˆå®‰è£…Daskï¼Œè¿˜æ˜¯å…ˆçœ‹çœ‹å¦‚ä½•åœ¨æ²¡æœ‰Daskçš„æƒ…å†µä¸‹å¤„ç†æ•°æ®ï¼Ÿ

```shellscript
pip install dask
# æˆ–è€…
conda install dask
```

```python
# å³ä½¿æ²¡æœ‰Daskï¼Œä½ ä¹Ÿå¯ä»¥æ‰‹åŠ¨æŒ‡å®šchunks
ds_chunked = ds.chunk({'time': 50, 'y': 100, 'x': 100})
```

```python
# æ‰“å¼€æ–‡ä»¶æ—¶è‡ªåŠ¨åˆ›å»ºchunks
ds = xr.open_dataset('../data/20101101.nc', chunks='auto')
```

```python
# æ£€æŸ¥Daskæ˜¯å¦å®‰è£…
try:
    import dask
    print(f"âœ… Daskå·²å®‰è£…ï¼Œç‰ˆæœ¬: {dask.__version__}")
    dask_available = True
except ImportError:
    print("âŒ Daskæœªå®‰è£…")
    dask_available = False

# æ£€æŸ¥æ•°æ®ç±»å‹
for var_name in ds.data_vars:
    var = ds[var_name]
    print(f"{var_name}: {type(var.data)}")
    print(f"  æ˜¯å¦æœ‰chunks: {hasattr(var.data, 'chunks')}")
```

```python
ds = xr.open_dataset('../data/20101101.nc', chunks='auto')
print(ds.chunks)
# è¾“å‡ºç±»ä¼¼: Frozen({'time': (50, 50, 50, 14), 'y': (100, 100, 100, 100, 80), 'x': (100, 100, 100, 100, 80)})
```

---

**User**

æˆ‘å®‰è£…daskåds.chunksè¾“å‡ºFrozen({'time': (174, 112), 'y': (360, 120), 'x': (360, 120)})ï¼Œæ˜¯ä¸æ˜¯è¯´åˆ†äº†ä¸¤ä¸ªå—ï¼Œç¬¬ä¸€ä¸ªå—å¤§å°ä¸º(174,360,360)ï¼Œç¬¬äºŒä¸ªå—ä¸º(112,120,120)ï¼Ÿé‚£dsçš„attributesé‡Œæ‰“å°å‡ºæ¥çš„chunks: 164 122åˆæ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ

---

**Cursor**

ä½ çš„ç†è§£æœ‰ä¸€ä¸ªå°è¯¯åŒºï¼è®©æˆ‘è¯¦ç»†è§£é‡ŠDask chunksçš„å®é™…å«ä¹‰ï¼Œè¿™æ¶‰åŠåˆ°**åˆ†å—å­˜å‚¨**å’Œ**ç»´åº¦åˆ†å—**çš„åŒºåˆ«ã€‚

## ï¿½ï¿½ Dask Chunksçš„çœŸå®å«ä¹‰

### ä½ çš„è¾“å‡ºåˆ†æ
```python
ds.chunks = Frozen({
    'time': (174, 112), 
    'y': (360, 120), 
    'x': (360, 120)
})
```

è¿™ä¸ªè¾“å‡º**ä¸æ˜¯**è¯´æœ‰ä¸¤ä¸ªå—ï¼Œè€Œæ˜¯è¯´**æ¯ä¸ªç»´åº¦éƒ½è¢«åˆ†æˆäº†å¤šä¸ªå—**ï¼š

### å®é™…çš„åˆ†å—ç»“æ„

```
æ—¶é—´ç»´åº¦ (time): 174 + 112 = 286ä¸ªæ—¶é—´æ­¥
  â†“
Yè½´ç»´åº¦ (y): 360 + 120 = 480ä¸ªåƒç´   
  â†“
Xè½´ç»´åº¦ (x): 360 + 120 = 480ä¸ªåƒç´ 
```

### çœŸæ­£çš„å—æ•°é‡è®¡ç®—

æ€»å—æ•° = æ—¶é—´å—æ•° Ã— Yå—æ•° Ã— Xå—æ•°
= 2 Ã— 2 Ã— 2 = **8ä¸ªå—**

æ¯ä¸ªå—çš„å¤§å°ï¼š
- å—1: (174, 360, 360)
- å—2: (174, 360, 120) 
- å—3: (174, 120, 360)
- å—4: (174, 120, 120)
- å—5: (112, 360, 360)
- å—6: (112, 360, 120)
- å—7: (112, 120, 360)
- å—8: (112, 120, 120)

## ğŸ” å…³äºattributesä¸­çš„chunksä¿¡æ¯

ä½ æåˆ°çš„"dsçš„attributesé‡Œæ‰“å°å‡ºæ¥çš„chunks: 164 122"å¯èƒ½æ˜¯ï¼š

1. **NetCDFæ–‡ä»¶æœ¬èº«çš„chunksä¿¡æ¯**ï¼ˆå­˜å‚¨ä¼˜åŒ–ï¼‰
2. **xarrayçš„é»˜è®¤chunksè®¾ç½®**
3. **ä¹‹å‰ç‰ˆæœ¬çš„chunksä¿¡æ¯**

è®©æˆ‘å¸®ä½ åˆ›å»ºä¸€ä¸ªè¯¦ç»†çš„éªŒè¯ä»£ç ï¼š

```python
# åœ¨ä½ çš„notebookä¸­æ·»åŠ è¿™ä¸ªcellæ¥è¯¦ç»†åˆ†æ

print("=== è¯¦ç»†Chunksåˆ†æ ===")
print(f"ds.chunks: {ds.chunks}")

# è®¡ç®—æ€»å—æ•°
total_chunks = 1
for dim, chunks in ds.chunks.items():
    chunk_count = len(chunks)
    total_chunks *= chunk_count
    print(f"{dim}ç»´åº¦: {chunk_count}ä¸ªå—, å¤§å°: {chunks}")

print(f"\næ€»å—æ•°: {total_chunks}")

# æ£€æŸ¥æ¯ä¸ªç»´åº¦çš„æ€»å¤§å°
print("\n=== ç»´åº¦å¤§å°éªŒè¯ ===")
for dim, chunks in ds.chunks.items():
    total_size = sum(chunks)
    actual_size = ds.dims[dim]
    print(f"{dim}: chunksæ€»å’Œ={total_size}, å®é™…å¤§å°={actual_size}, åŒ¹é…={total_size == actual_size}")

# æ£€æŸ¥NetCDFæ–‡ä»¶çš„åŸå§‹chunksä¿¡æ¯
print("\n=== NetCDFæ–‡ä»¶åŸå§‹ä¿¡æ¯ ===")
try:
    import netCDF4
    nc = netCDF4.Dataset('../data/20101101.nc', 'r')
    for var_name in nc.variables:
        var = nc.variables[var_name]
        if hasattr(var, 'chunking'):
            print(f"{var_name}: {var.chunking()}")
    nc.close()
except Exception as e:
    print(f"æ— æ³•è¯»å–NetCDFæ–‡ä»¶ä¿¡æ¯: {e}")

# æ¼”ç¤ºå¦‚ä½•è®¿é—®ç‰¹å®šå—
print("\n=== è®¿é—®ç‰¹å®šå—ç¤ºä¾‹ ===")
if ds.data_vars:
    first_var = list(ds.data_vars.keys())[0]
    var = ds[first_var]
    
    # è®¿é—®ç¬¬ä¸€ä¸ªæ—¶é—´å—
    print("ç¬¬ä¸€ä¸ªæ—¶é—´å— (å‰174ä¸ªæ—¶é—´æ­¥):")
    first_time_chunk = var.isel(time=slice(0, 174))
    print(f"å½¢çŠ¶: {first_time_chunk.shape}")
    
    # è®¿é—®ç¬¬äºŒä¸ªæ—¶é—´å—  
    print("ç¬¬äºŒä¸ªæ—¶é—´å— (å112ä¸ªæ—¶é—´æ­¥):")
    second_time_chunk = var.isel(time=slice(174, 286))
    print(f"å½¢çŠ¶: {second_time_chunk.shape}")
```

## ğŸ“Š å®é™…çš„æ•°æ®è®¿é—®æ–¹å¼

### æŒ‰å—è®¿é—®æ•°æ®
```python
# è®¿é—®ç¬¬ä¸€ä¸ªå®Œæ•´çš„å— (174, 360, 360)
block1 = ds.isel(time=slice(0, 174), y=slice(0, 360), x=slice(0, 360))

# è®¿é—®æœ€åä¸€ä¸ªå— (112, 120, 120)  
block8 = ds.isel(time=slice(174, 286), y=slice(360, 480), x=slice(360, 480))
```

### ä¸ºä»€ä¹ˆè¿™æ ·åˆ†å—ï¼Ÿ

1. **å†…å­˜ä¼˜åŒ–**: æ¯ä¸ªå—å¤§å°é€‚ä¸­ï¼Œä¸ä¼šå ç”¨è¿‡å¤šå†…å­˜
2. **å¹¶è¡Œå¤„ç†**: å¯ä»¥åŒæ—¶å¤„ç†å¤šä¸ªå—
3. **I/Oæ•ˆç‡**: åªè¯»å–éœ€è¦çš„æ•°æ®å—

## ï¿½ï¿½ å…³é”®ç†è§£ç‚¹

1. **chunkså…ƒç»„ä¸­çš„æ•°å­—** = æ¯ä¸ªå—çš„å¤§å°
2. **chunkså…ƒç»„çš„é•¿åº¦** = è¯¥ç»´åº¦è¢«åˆ†æˆçš„å—æ•°
3. **æ€»å—æ•°** = æ‰€æœ‰ç»´åº¦å—æ•°çš„ä¹˜ç§¯
4. **æ¯ä¸ªå—çš„å®é™…å¤§å°** = å„ç»´åº¦å—å¤§å°çš„ç»„åˆ

## ï¿½ï¿½ å®é™…åº”ç”¨å»ºè®®

```python
# æŸ¥çœ‹å…·ä½“çš„æ•°æ®å—ä¿¡æ¯
print("=== æ•°æ®å—è¯¦ç»†ä¿¡æ¯ ===")
for i, (dim, chunks) in enumerate(ds.chunks.items()):
    print(f"ç»´åº¦ {dim}:")
    for j, chunk_size in enumerate(chunks):
        start_idx = sum(chunks[:j])
        end_idx = start_idx + chunk_size
        print(f"  å—{j+1}: ç´¢å¼• {start_idx}:{end_idx}, å¤§å° {chunk_size}")
```

è¿™æ ·ä½ å°±èƒ½å®Œå…¨ç†è§£Daskæ˜¯å¦‚ä½•ç»„ç»‡ä½ çš„é›·è¾¾æ•°æ®çš„äº†ï¼æ¯ä¸ªç»´åº¦éƒ½è¢«æ™ºèƒ½åœ°åˆ†å—ï¼Œä»¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æ•ˆç‡ã€‚

```python
ds.chunks = Frozen({
    'time': (174, 112), 
    'y': (360, 120), 
    'x': (360, 120)
})
```

```plaintext
æ—¶é—´ç»´åº¦ (time): 174 + 112 = 286ä¸ªæ—¶é—´æ­¥
  â†“
Yè½´ç»´åº¦ (y): 360 + 120 = 480ä¸ªåƒç´   
  â†“
Xè½´ç»´åº¦ (x): 360 + 120 = 480ä¸ªåƒç´ 
```

```python
# åœ¨ä½ çš„notebookä¸­æ·»åŠ è¿™ä¸ªcellæ¥è¯¦ç»†åˆ†æ

print("=== è¯¦ç»†Chunksåˆ†æ ===")
print(f"ds.chunks: {ds.chunks}")

# è®¡ç®—æ€»å—æ•°
total_chunks = 1
for dim, chunks in ds.chunks.items():
    chunk_count = len(chunks)
    total_chunks *= chunk_count
    print(f"{dim}ç»´åº¦: {chunk_count}ä¸ªå—, å¤§å°: {chunks}")

print(f"\næ€»å—æ•°: {total_chunks}")

# æ£€æŸ¥æ¯ä¸ªç»´åº¦çš„æ€»å¤§å°
print("\n=== ç»´åº¦å¤§å°éªŒè¯ ===")
for dim, chunks in ds.chunks.items():
    total_size = sum(chunks)
    actual_size = ds.dims[dim]
    print(f"{dim}: chunksæ€»å’Œ={total_size}, å®é™…å¤§å°={actual_size}, åŒ¹é…={total_size == actual_size}")

# æ£€æŸ¥NetCDFæ–‡ä»¶çš„åŸå§‹chunksä¿¡æ¯
print("\n=== NetCDFæ–‡ä»¶åŸå§‹ä¿¡æ¯ ===")
try:
    import netCDF4
    nc = netCDF4.Dataset('../data/20101101.nc', 'r')
    for var_name in nc.variables:
        var = nc.variables[var_name]
        if hasattr(var, 'chunking'):
            print(f"{var_name}: {var.chunking()}")
    nc.close()
except Exception as e:
    print(f"æ— æ³•è¯»å–NetCDFæ–‡ä»¶ä¿¡æ¯: {e}")

# æ¼”ç¤ºå¦‚ä½•è®¿é—®ç‰¹å®šå—
print("\n=== è®¿é—®ç‰¹å®šå—ç¤ºä¾‹ ===")
if ds.data_vars:
    first_var = list(ds.data_vars.keys())[0]
    var = ds[first_var]
    
    # è®¿é—®ç¬¬ä¸€ä¸ªæ—¶é—´å—
    print("ç¬¬ä¸€ä¸ªæ—¶é—´å— (å‰174ä¸ªæ—¶é—´æ­¥):")
    first_time_chunk = var.isel(time=slice(0, 174))
    print(f"å½¢çŠ¶: {first_time_chunk.shape}")
    
    # è®¿é—®ç¬¬äºŒä¸ªæ—¶é—´å—  
    print("ç¬¬äºŒä¸ªæ—¶é—´å— (å112ä¸ªæ—¶é—´æ­¥):")
    second_time_chunk = var.isel(time=slice(174, 286))
    print(f"å½¢çŠ¶: {second_time_chunk.shape}")
```

```python
# è®¿é—®ç¬¬ä¸€ä¸ªå®Œæ•´çš„å— (174, 360, 360)
block1 = ds.isel(time=slice(0, 174), y=slice(0, 360), x=slice(0, 360))

# è®¿é—®æœ€åä¸€ä¸ªå— (112, 120, 120)  
block8 = ds.isel(time=slice(174, 286), y=slice(360, 480), x=slice(360, 480))
```

```python
# æŸ¥çœ‹å…·ä½“çš„æ•°æ®å—ä¿¡æ¯
print("=== æ•°æ®å—è¯¦ç»†ä¿¡æ¯ ===")
for i, (dim, chunks) in enumerate(ds.chunks.items()):
    print(f"ç»´åº¦ {dim}:")
    for j, chunk_size in enumerate(chunks):
        start_idx = sum(chunks[:j])
        end_idx = start_idx + chunk_size
        print(f"  å—{j+1}: ç´¢å¼• {start_idx}:{end_idx}, å¤§å° {chunk_size}")
```

---

**User**

å¦‚æœè¯´æˆ‘ä¸€ä¸ªncæ–‡ä»¶åˆ†æˆäº†8ä¸ªå—æ¥è¯»å–ï¼Œxarrayæ˜¯ä¸æ˜¯ä¼šè‡ªåŠ¨å¯ç”¨å¤šä¸ªçº¿ç¨‹æ¥è¯»å–ï¼Œæœ€ååˆå¹¶åˆ°ä¸€èµ·ä»¥æé«˜è¯»å–çš„é€Ÿåº¦ï¼Ÿ

---

**Cursor**

ä½ çš„ç†è§£å¾ˆå‡†ç¡®ï¼Daskç¡®å®ä¼šè‡ªåŠ¨å¯ç”¨å¤šçº¿ç¨‹æ¥å¹¶è¡Œè¯»å–æ•°æ®å—ï¼Œä½†è®©æˆ‘è¯¦ç»†è§£é‡Šä¸€ä¸‹è¿™ä¸ªè¿‡ç¨‹å’Œç›¸å…³çš„ä¼˜åŒ–æœºåˆ¶ã€‚

## ğŸš€ Daskçš„å¹¶è¡Œè¯»å–æœºåˆ¶

### è‡ªåŠ¨å¹¶è¡ŒåŒ–
æ˜¯çš„ï¼Œå½“ä½ ä½¿ç”¨Daskæ—¶ï¼Œxarrayä¼šè‡ªåŠ¨ï¼š
1. **å¹¶è¡Œè¯»å–**å¤šä¸ªæ•°æ®å—
2. **å¤šçº¿ç¨‹å¤„ç†**I/Oæ“ä½œ
3. **å»¶è¿Ÿè®¡ç®—**ç›´åˆ°çœŸæ­£éœ€è¦æ•°æ®æ—¶æ‰è¯»å–
4. **æ™ºèƒ½åˆå¹¶**æ•°æ®å—

### å®é™…çš„å·¥ä½œæµç¨‹

```python
# å½“ä½ æ‰§è¡Œè¿™ä¸ªæ“ä½œæ—¶ï¼š
ds = xr.open_dataset('file.nc', chunks='auto')
result = ds['reflectivity'].mean(dim='time')

# Daskå†…éƒ¨ä¼šï¼š
# 1. åˆ†æéœ€è¦å“ªäº›æ•°æ®å—
# 2. å¹¶è¡Œè¯»å–è¿™äº›å—
# 3. åœ¨å†…å­˜ä¸­åˆå¹¶
# 4. æ‰§è¡Œè®¡ç®—
```

## ğŸ”§ éªŒè¯å¹¶è¡Œè¯»å–

è®©æˆ‘åœ¨ä½ çš„notebookä¸­æ·»åŠ ä¸€äº›ä»£ç æ¥éªŒè¯è¿™ä¸ªæœºåˆ¶ï¼š

```python
# åœ¨ä½ çš„notebookä¸­æ·»åŠ è¿™ä¸ªcell

import time
import dask
from dask.diagnostics import ProgressBar

print("=== Daskå¹¶è¡Œè¯»å–éªŒè¯ ===")

# 1. æ£€æŸ¥Daské…ç½®
print(f"Daskçº¿ç¨‹æ•°: {dask.config.get('num_workers', 'auto')}")
print(f"Daskçº¿ç¨‹æ± å¤§å°: {dask.config.get('threads_per_worker', 'auto')}")

# 2. æµ‹è¯•è¯»å–æ€§èƒ½
print("\n=== æ€§èƒ½æµ‹è¯• ===")

# æ–¹æ³•1: ä¸ä½¿ç”¨chunks (ä¸²è¡Œè¯»å–)
start_time = time.time()
ds_no_chunks = xr.open_dataset('../data/20101101.nc')
if ds_no_chunks.data_vars:
    first_var = list(ds_no_chunks.data_vars.keys())[0]
    data_no_chunks = ds_no_chunks[first_var].values
serial_time = time.time() - start_time
print(f"ä¸²è¡Œè¯»å–æ—¶é—´: {serial_time:.2f}ç§’")

# æ–¹æ³•2: ä½¿ç”¨chunks (å¹¶è¡Œè¯»å–)
start_time = time.time()
ds_chunked = xr.open_dataset('../data/20101101.nc', chunks='auto')
if ds_chunked.data_vars:
    first_var = list(ds_chunked.data_vars.keys())[0]
    data_chunked = ds_chunked[first_var].compute()
parallel_time = time.time() - start_time
print(f"å¹¶è¡Œè¯»å–æ—¶é—´: {parallel_time:.2f}ç§’")

print(f"æ€§èƒ½æå‡: {serial_time/parallel_time:.2f}x")

# 3. æ˜¾ç¤ºè¯»å–è¿›åº¦
print("\n=== å¸¦è¿›åº¦çš„è¯»å– ===")
with ProgressBar():
    # è¿™ä¼šæ˜¾ç¤ºè¯»å–è¿›åº¦æ¡
    result = ds_chunked[first_var].mean(dim='time').compute()
    print(f"è®¡ç®—å®Œæˆï¼Œç»“æœå½¢çŠ¶: {result.shape}")
```

## âš™ï¸ ä¼˜åŒ–Daskæ€§èƒ½

### 1. è°ƒæ•´çº¿ç¨‹æ•°
```python
# è®¾ç½®Daské…ç½®
import dask
dask.config.set({
    'num_workers': 4,  # å·¥ä½œçº¿ç¨‹æ•°
    'threads_per_worker': 2,  # æ¯ä¸ªå·¥ä½œçº¿ç¨‹çš„çº¿ç¨‹æ•°
    'memory_limit': '2GB'  # å†…å­˜é™åˆ¶
})
```

### 2. ä¼˜åŒ–chunkså¤§å°
```python
# æ ¹æ®ä½ çš„æ•°æ®ç‰¹ç‚¹è°ƒæ•´chunks
ds_optimized = xr.open_dataset('../data/20101101.nc', 
                              chunks={'time': 50, 'y': 100, 'x': 100})
```

### 3. ä½¿ç”¨ä¸åŒçš„è°ƒåº¦å™¨
```python
# ä½¿ç”¨çº¿ç¨‹æ± è°ƒåº¦å™¨ï¼ˆé€‚åˆI/Oå¯†é›†å‹ä»»åŠ¡ï¼‰
from dask.threaded import get
result = ds['reflectivity'].mean().compute(scheduler='threads')

# ä½¿ç”¨è¿›ç¨‹æ± è°ƒåº¦å™¨ï¼ˆé€‚åˆCPUå¯†é›†å‹ä»»åŠ¡ï¼‰
from dask.multiprocessing import get
result = ds['reflectivity'].mean().compute(scheduler='processes')
```

## ğŸ“Š å®é™…çš„å†…å­˜ä½¿ç”¨æƒ…å†µ

```python
# ç›‘æ§å†…å­˜ä½¿ç”¨
import psutil
import os

def monitor_memory():
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024  # MB

print("=== å†…å­˜ä½¿ç”¨ç›‘æ§ ===")
print(f"åˆå§‹å†…å­˜: {monitor_memory():.1f} MB")

# è¯»å–æ•°æ®
ds = xr.open_dataset('../data/20101101.nc', chunks='auto')
print(f"æ‰“å¼€æ•°æ®é›†å: {monitor_memory():.1f} MB")

# è®¡ç®—æŸä¸ªç»Ÿè®¡é‡
if ds.data_vars:
    first_var = list(ds.data_vars.keys())[0]
    result = ds[first_var].mean(dim='time').compute()
    print(f"è®¡ç®—å®Œæˆå: {monitor_memory():.1f} MB")
    print(f"ç»“æœå½¢çŠ¶: {result.shape}")
```

## ï¿½ï¿½ é’ˆå¯¹TAASRAD19é¡¹ç›®çš„ä¼˜åŒ–å»ºè®®

### 1. é›·è¾¾æ•°æ®çš„ç‰¹ç‚¹
- **æ—¶é—´åºåˆ—**: è¿ç»­çš„æ—¶é—´æ­¥
- **ç©ºé—´æ•°æ®**: å¤§å°ºå¯¸å›¾åƒ (480Ã—480)
- **I/Oå¯†é›†å‹**: ä¸»è¦æ˜¯ç£ç›˜è¯»å–

### 2. æœ€ä½³å®è·µ
```python
# é’ˆå¯¹é›·è¾¾æ•°æ®ä¼˜åŒ–çš„é…ç½®
ds_radar = xr.open_dataset('../data/20101101.nc', 
                          chunks={'time': 20, 'y': 120, 'x': 120})

# ä½¿ç”¨çº¿ç¨‹è°ƒåº¦å™¨ï¼ˆé€‚åˆI/Oï¼‰
result = ds_radar['reflectivity'].mean(dim='time').compute(scheduler='threads')
```

### 3. æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶
```python
# å¹¶è¡Œå¤„ç†å¤šä¸ªNCæ–‡ä»¶
import glob
from dask.delayed import delayed

@delayed
def process_file(filename):
    ds = xr.open_dataset(filename, chunks='auto')
    return ds['reflectivity'].mean(dim='time')

# è·å–æ‰€æœ‰NCæ–‡ä»¶
nc_files = glob.glob('../data/*.nc')

# å¹¶è¡Œå¤„ç†
results = [process_file(f) for f in nc_files]
final_result = dask.compute(*results)
```

## ğŸ’¡ å…³é”®ç†è§£ç‚¹

1. **è‡ªåŠ¨å¹¶è¡ŒåŒ–**: Daskä¼šè‡ªåŠ¨å¹¶è¡Œè¯»å–æ•°æ®å—
2. **å»¶è¿Ÿè®¡ç®—**: åªæœ‰åœ¨è°ƒç”¨`.compute()`æ—¶æ‰çœŸæ­£è¯»å–æ•°æ®
3. **å†…å­˜ä¼˜åŒ–**: ä¸ä¼šä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜
4. **å¯é…ç½®æ€§**: å¯ä»¥è°ƒæ•´çº¿ç¨‹æ•°ã€chunkså¤§å°ç­‰å‚æ•°
5. **è¿›åº¦ç›‘æ§**: å¯ä»¥æ˜¾ç¤ºè¯»å–å’Œè®¡ç®—è¿›åº¦

## å®é™…æ•ˆæœ

ä½¿ç”¨Daskåï¼Œä½ åº”è¯¥èƒ½çœ‹åˆ°ï¼š
- **æ›´å¿«çš„è¯»å–é€Ÿåº¦**ï¼ˆç‰¹åˆ«æ˜¯å¤§æ–‡ä»¶ï¼‰
- **æ›´ä½çš„å†…å­˜ä½¿ç”¨**ï¼ˆæŒ‰éœ€åŠ è½½ï¼‰
- **æ›´å¥½çš„å¯æ‰©å±•æ€§**ï¼ˆå¯ä»¥å¤„ç†æ›´å¤§çš„æ•°æ®é›†ï¼‰
- **è¿›åº¦æ¡æ˜¾ç¤º**ï¼ˆäº†è§£å¤„ç†è¿›åº¦ï¼‰

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆDaskåœ¨æ°”è±¡æ•°æ®å¤„ç†ä¸­å¦‚æ­¤å—æ¬¢è¿çš„åŸå› ï¼å®ƒè®©å¤„ç†å¤§å‹é›·è¾¾æ•°æ®é›†å˜å¾—æ—¢é«˜æ•ˆåˆå†…å­˜å‹å¥½ã€‚

```python
# å½“ä½ æ‰§è¡Œè¿™ä¸ªæ“ä½œæ—¶ï¼š
ds = xr.open_dataset('file.nc', chunks='auto')
result = ds['reflectivity'].mean(dim='time')

# Daskå†…éƒ¨ä¼šï¼š
# 1. åˆ†æéœ€è¦å“ªäº›æ•°æ®å—
# 2. å¹¶è¡Œè¯»å–è¿™äº›å—
# 3. åœ¨å†…å­˜ä¸­åˆå¹¶
# 4. æ‰§è¡Œè®¡ç®—
```

```python
# åœ¨ä½ çš„notebookä¸­æ·»åŠ è¿™ä¸ªcell

import time
import dask
from dask.diagnostics import ProgressBar

print("=== Daskå¹¶è¡Œè¯»å–éªŒè¯ ===")

# 1. æ£€æŸ¥Daské…ç½®
print(f"Daskçº¿ç¨‹æ•°: {dask.config.get('num_workers', 'auto')}")
print(f"Daskçº¿ç¨‹æ± å¤§å°: {dask.config.get('threads_per_worker', 'auto')}")

# 2. æµ‹è¯•è¯»å–æ€§èƒ½
print("\n=== æ€§èƒ½æµ‹è¯• ===")

# æ–¹æ³•1: ä¸ä½¿ç”¨chunks (ä¸²è¡Œè¯»å–)
start_time = time.time()
ds_no_chunks = xr.open_dataset('../data/20101101.nc')
if ds_no_chunks.data_vars:
    first_var = list(ds_no_chunks.data_vars.keys())[0]
    data_no_chunks = ds_no_chunks[first_var].values
serial_time = time.time() - start_time
print(f"ä¸²è¡Œè¯»å–æ—¶é—´: {serial_time:.2f}ç§’")

# æ–¹æ³•2: ä½¿ç”¨chunks (å¹¶è¡Œè¯»å–)
start_time = time.time()
ds_chunked = xr.open_dataset('../data/20101101.nc', chunks='auto')
if ds_chunked.data_vars:
    first_var = list(ds_chunked.data_vars.keys())[0]
    data_chunked = ds_chunked[first_var].compute()
parallel_time = time.time() - start_time
print(f"å¹¶è¡Œè¯»å–æ—¶é—´: {parallel_time:.2f}ç§’")

print(f"æ€§èƒ½æå‡: {serial_time/parallel_time:.2f}x")

# 3. æ˜¾ç¤ºè¯»å–è¿›åº¦
print("\n=== å¸¦è¿›åº¦çš„è¯»å– ===")
with ProgressBar():
    # è¿™ä¼šæ˜¾ç¤ºè¯»å–è¿›åº¦æ¡
    result = ds_chunked[first_var].mean(dim='time').compute()
    print(f"è®¡ç®—å®Œæˆï¼Œç»“æœå½¢çŠ¶: {result.shape}")
```

```python
# è®¾ç½®Daské…ç½®
import dask
dask.config.set({
    'num_workers': 4,  # å·¥ä½œçº¿ç¨‹æ•°
    'threads_per_worker': 2,  # æ¯ä¸ªå·¥ä½œçº¿ç¨‹çš„çº¿ç¨‹æ•°
    'memory_limit': '2GB'  # å†…å­˜é™åˆ¶
})
```

```python
# æ ¹æ®ä½ çš„æ•°æ®ç‰¹ç‚¹è°ƒæ•´chunks
ds_optimized = xr.open_dataset('../data/20101101.nc', 
                              chunks={'time': 50, 'y': 100, 'x': 100})
```

```python
# ä½¿ç”¨çº¿ç¨‹æ± è°ƒåº¦å™¨ï¼ˆé€‚åˆI/Oå¯†é›†å‹ä»»åŠ¡ï¼‰
from dask.threaded import get
result = ds['reflectivity'].mean().compute(scheduler='threads')

# ä½¿ç”¨è¿›ç¨‹æ± è°ƒåº¦å™¨ï¼ˆé€‚åˆCPUå¯†é›†å‹ä»»åŠ¡ï¼‰
from dask.multiprocessing import get
result = ds['reflectivity'].mean().compute(scheduler='processes')
```

```python
# ç›‘æ§å†…å­˜ä½¿ç”¨
import psutil
import os

def monitor_memory():
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024  # MB

print("=== å†…å­˜ä½¿ç”¨ç›‘æ§ ===")
print(f"åˆå§‹å†…å­˜: {monitor_memory():.1f} MB")

# è¯»å–æ•°æ®
ds = xr.open_dataset('../data/20101101.nc', chunks='auto')
print(f"æ‰“å¼€æ•°æ®é›†å: {monitor_memory():.1f} MB")

# è®¡ç®—æŸä¸ªç»Ÿè®¡é‡
if ds.data_vars:
    first_var = list(ds.data_vars.keys())[0]
    result = ds[first_var].mean(dim='time').compute()
    print(f"è®¡ç®—å®Œæˆå: {monitor_memory():.1f} MB")
    print(f"ç»“æœå½¢çŠ¶: {result.shape}")
```

```python
# é’ˆå¯¹é›·è¾¾æ•°æ®ä¼˜åŒ–çš„é…ç½®
ds_radar = xr.open_dataset('../data/20101101.nc', 
                          chunks={'time': 20, 'y': 120, 'x': 120})

# ä½¿ç”¨çº¿ç¨‹è°ƒåº¦å™¨ï¼ˆé€‚åˆI/Oï¼‰
result = ds_radar['reflectivity'].mean(dim='time').compute(scheduler='threads')
```

```python
# å¹¶è¡Œå¤„ç†å¤šä¸ªNCæ–‡ä»¶
import glob
from dask.delayed import delayed

@delayed
def process_file(filename):
    ds = xr.open_dataset(filename, chunks='auto')
    return ds['reflectivity'].mean(dim='time')

# è·å–æ‰€æœ‰NCæ–‡ä»¶
nc_files = glob.glob('../data/*.nc')

# å¹¶è¡Œå¤„ç†
results = [process_file(f) for f in nc_files]
final_result = dask.compute(*results)
```

---

**User**

æœ‰çš„æ—¶å€™ncæ–‡ä»¶å¯èƒ½æ¯”è¾ƒå¤§ï¼Œæˆ‘å¯èƒ½åªæƒ³è¯»å–ä¸€éƒ¨åˆ†æ•°æ®ï¼ˆæ¯”å¦‚ä¸€ä¸ªchunkï¼‰å¹¶åŠ è½½åˆ°å†…å­˜é‡Œç„¶ååˆ†æï¼Œæœ‰æ²¡æœ‰ç®€å•æ–¹æ³•ä¸€æ¬¡è¯»å–ä¸€ä¸ªchunkçš„æ•°æ®ï¼Ÿ

---

**Cursor**

ç»å¯¹æœ‰ï¼è¿™æ˜¯ä¸€ä¸ªéå¸¸å®ç”¨çš„éœ€æ±‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§å‹NCæ–‡ä»¶æ—¶ã€‚è®©æˆ‘ä¸ºä½ æä¾›å‡ ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•æ¥ä¸€æ¬¡è¯»å–ä¸€ä¸ªchunkçš„æ•°æ®ã€‚

## ğŸ¯ æ–¹æ³•1: ä½¿ç”¨ç´¢å¼•åˆ‡ç‰‡è¯»å–å•ä¸ªchunk

```python
# åœ¨ä½ çš„notebookä¸­æ·»åŠ è¿™ä¸ªcell

print("=== æ–¹æ³•1: ç´¢å¼•åˆ‡ç‰‡è¯»å–å•ä¸ªchunk ===")

# å‡è®¾ä½ æƒ³è¯»å–ç¬¬ä¸€ä¸ªchunk
# æ ¹æ®ä¹‹å‰çš„chunksä¿¡æ¯: time(174, 112), y(360, 120), x(360, 120)

# è¯»å–ç¬¬ä¸€ä¸ªå®Œæ•´çš„chunk (174, 360, 360)
chunk1 = ds.isel(
    time=slice(0, 174),      # ç¬¬ä¸€ä¸ªæ—¶é—´å—
    y=slice(0, 360),         # ç¬¬ä¸€ä¸ªYå—  
    x=slice(0, 360)          # ç¬¬ä¸€ä¸ªXå—
)

print(f"ç¬¬ä¸€ä¸ªchunkå½¢çŠ¶: {chunk1[list(ds.data_vars.keys())[0]].shape}")
print(f"ç¬¬ä¸€ä¸ªchunkå¤§å°: {chunk1[list(ds.data_vars.keys())[0]].nbytes / 1024 / 1024:.2f} MB")

# è¯»å–æœ€åä¸€ä¸ªchunk (112, 120, 120)
chunk_last = ds.isel(
    time=slice(174, 286),    # ç¬¬äºŒä¸ªæ—¶é—´å—
    y=slice(360, 480),       # ç¬¬äºŒä¸ªYå—
    x=slice(360, 480)        # ç¬¬äºŒä¸ªXå—
)

print(f"æœ€åä¸€ä¸ªchunkå½¢çŠ¶: {chunk_last[list(ds.data_vars.keys())[0]].shape}")
```

## ï¿½ï¿½ æ–¹æ³•2: è‡ªåŠ¨è®¡ç®—chunkè¾¹ç•Œ

```python
# æ–¹æ³•2: è‡ªåŠ¨è®¡ç®—chunkè¾¹ç•Œ
print("\n=== æ–¹æ³•2: è‡ªåŠ¨è®¡ç®—chunkè¾¹ç•Œ ===")

def get_chunk_boundaries(ds, chunk_index):
    """è·å–æŒ‡å®šchunkçš„è¾¹ç•Œç´¢å¼•"""
    boundaries = {}
    
    for dim, chunks in ds.chunks.items():
        start_idx = 0
        for i, chunk_size in enumerate(chunks):
            if i == chunk_index:
                boundaries[dim] = (start_idx, start_idx + chunk_size)
                break
            start_idx += chunk_size
    
    return boundaries

# è·å–ç¬¬ä¸€ä¸ªchunkçš„è¾¹ç•Œ
chunk_0_boundaries = get_chunk_boundaries(ds, 0)
print(f"ç¬¬ä¸€ä¸ªchunkè¾¹ç•Œ: {chunk_0_boundaries}")

# ä½¿ç”¨è¾¹ç•Œè¯»å–æ•°æ®
if chunk_0_boundaries:
    chunk_data = ds.isel({
        dim: slice(start, end) 
        for dim, (start, end) in chunk_0_boundaries.items()
    })
    print(f"è¯»å–çš„chunkå½¢çŠ¶: {chunk_data[list(ds.data_vars.keys())[0]].shape}")
```

## ğŸ“Š æ–¹æ³•3: æŒ‰æ—¶é—´chunkè¯»å–

```python
# æ–¹æ³•3: æŒ‰æ—¶é—´chunkè¯»å–ï¼ˆæœ€å¸¸ç”¨ï¼‰
print("\n=== æ–¹æ³•3: æŒ‰æ—¶é—´chunkè¯»å– ===")

# è¯»å–ç¬¬ä¸€ä¸ªæ—¶é—´chunkçš„æ‰€æœ‰ç©ºé—´æ•°æ®
time_chunk_1 = ds.isel(time=slice(0, 174))
print(f"ç¬¬ä¸€ä¸ªæ—¶é—´chunkå½¢çŠ¶: {time_chunk_1[list(ds.data_vars.keys())[0]].shape}")

# è¯»å–ç¬¬äºŒä¸ªæ—¶é—´chunkçš„æ‰€æœ‰ç©ºé—´æ•°æ®  
time_chunk_2 = ds.isel(time=slice(174, 286))
print(f"ç¬¬äºŒä¸ªæ—¶é—´chunkå½¢çŠ¶: {time_chunk_2[list(ds.data_vars.keys())[0]].shape}")

# è¯»å–ç‰¹å®šæ—¶é—´èŒƒå›´
time_subset = ds.isel(time=slice(0, 50))  # å‰50ä¸ªæ—¶é—´æ­¥
print(f"å‰50ä¸ªæ—¶é—´æ­¥å½¢çŠ¶: {time_subset[list(ds.data_vars.keys())[0]].shape}")
```

## ğŸ¨ æ–¹æ³•4: æŒ‰ç©ºé—´chunkè¯»å–

```python
# æ–¹æ³•4: æŒ‰ç©ºé—´chunkè¯»å–
print("\n=== æ–¹æ³•4: æŒ‰ç©ºé—´chunkè¯»å– ===")

# è¯»å–ç¬¬ä¸€ä¸ªç©ºé—´chunkçš„æ‰€æœ‰æ—¶é—´æ•°æ®
spatial_chunk_1 = ds.isel(y=slice(0, 360), x=slice(0, 360))
print(f"ç¬¬ä¸€ä¸ªç©ºé—´chunkå½¢çŠ¶: {spatial_chunk_1[list(ds.data_vars.keys())[0]].shape}")

# è¯»å–ä¸­å¿ƒåŒºåŸŸ
center_chunk = ds.isel(
    y=slice(120, 360),  # ä¸­å¿ƒYåŒºåŸŸ
    x=slice(120, 360)   # ä¸­å¿ƒXåŒºåŸŸ
)
print(f"ä¸­å¿ƒåŒºåŸŸchunkå½¢çŠ¶: {center_chunk[list(ds.data_vars.keys())[0]].shape}")
```

## âš¡ æ–¹æ³•5: ä½¿ç”¨Daskçš„chunkè¿­ä»£

```python
# æ–¹æ³•5: ä½¿ç”¨Daskçš„chunkè¿­ä»£
print("\n=== æ–¹æ³•5: Dask chunkè¿­ä»£ ===")

def process_chunks_sequentially(ds, max_chunks=3):
    """é€ä¸ªå¤„ç†chunksï¼Œé¿å…å†…å­˜æº¢å‡º"""
    results = []
    
    if ds.data_vars:
        first_var = list(ds.data_vars.keys())[0]
        var = ds[first_var]
        
        # æŒ‰æ—¶é—´chunkå¤„ç†
        time_chunks = ds.chunks['time']
        start_idx = 0
        
        for i, chunk_size in enumerate(time_chunks[:max_chunks]):
            end_idx = start_idx + chunk_size
            
            print(f"å¤„ç†æ—¶é—´chunk {i+1}: {start_idx}:{end_idx}")
            
            # è¯»å–å½“å‰chunk
            current_chunk = ds.isel(time=slice(start_idx, end_idx))
            chunk_data = current_chunk[first_var]
            
            # è®¡ç®—ç»Ÿè®¡é‡
            chunk_mean = chunk_data.mean(dim='time')
            results.append(chunk_mean)
            
            print(f"  chunkå½¢çŠ¶: {chunk_data.shape}")
            print(f"  chunkå‡å€¼: {chunk_mean.mean().values:.2f}")
            
            start_idx = end_idx
    
    return results

# å¤„ç†å‰3ä¸ªæ—¶é—´chunks
chunk_results = process_chunks_sequentially(ds, max_chunks=2)
print(f"å¤„ç†äº† {len(chunk_results)} ä¸ªchunks")
```

## ğŸ¯ æ–¹æ³•6: æ™ºèƒ½chunké€‰æ‹©

```python
# æ–¹æ³•6: æ™ºèƒ½chunké€‰æ‹©
print("\n=== æ–¹æ³•6: æ™ºèƒ½chunké€‰æ‹© ===")

def read_smart_chunk(ds, target_memory_mb=100):
    """æ ¹æ®ç›®æ ‡å†…å­˜å¤§å°æ™ºèƒ½é€‰æ‹©chunk"""
    
    if ds.data_vars:
        first_var = list(ds.data_vars.keys())[0]
        var = ds[first_var]
        
        # è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„å†…å­˜å¤§å°
        time_chunks = ds.chunks['time']
        spatial_size = ds.dims['y'] * ds.dims['x']
        bytes_per_pixel = var.dtype.itemsize
        bytes_per_timestep = spatial_size * bytes_per_pixel
        
        # è®¡ç®—èƒ½å®¹çº³å¤šå°‘ä¸ªæ—¶é—´æ­¥
        max_timesteps = int(target_memory_mb * 1024 * 1024 / bytes_per_timestep)
        
        print(f"ç›®æ ‡å†…å­˜: {target_memory_mb} MB")
        print(f"æ¯ä¸ªæ—¶é—´æ­¥å¤§å°: {bytes_per_timestep / 1024 / 1024:.2f} MB")
        print(f"å»ºè®®æ—¶é—´æ­¥æ•°: {max_timesteps}")
        
        # é€‰æ‹©åˆé€‚çš„æ—¶é—´èŒƒå›´
        actual_timesteps = min(max_timesteps, time_chunks[0])
        smart_chunk = ds.isel(time=slice(0, actual_timesteps))
        
        print(f"å®é™…è¯»å–å½¢çŠ¶: {smart_chunk[first_var].shape}")
        print(f"å®é™…å†…å­˜ä½¿ç”¨: {smart_chunk[first_var].nbytes / 1024 / 1024:.2f} MB")
        
        return smart_chunk

# è¯»å–100MBçš„chunk
smart_chunk = read_smart_chunk(ds, target_memory_mb=100)
```

## ğŸ’¡ å®é™…åº”ç”¨å»ºè®®

### 1. æ ¹æ®åˆ†æéœ€æ±‚é€‰æ‹©æ–¹æ³•
```python
# æ—¶é—´åºåˆ—åˆ†æ - æŒ‰æ—¶é—´chunkè¯»å–
time_analysis_chunk = ds.isel(time=slice(0, 50))

# ç©ºé—´åˆ†æ - æŒ‰ç©ºé—´chunkè¯»å–  
spatial_analysis_chunk = ds.isel(y=slice(0, 240), x=slice(0, 240))

# ç‰¹å®šåŒºåŸŸåˆ†æ - è‡ªå®šä¹‰åŒºåŸŸ
region_chunk = ds.isel(
    time=slice(0, 100),
    y=slice(100, 300), 
    x=slice(100, 300)
)
```

### 2. å†…å­˜ç›‘æ§
```python
import psutil
import os

def monitor_chunk_memory(chunk):
    """ç›‘æ§chunkçš„å†…å­˜ä½¿ç”¨"""
    process = psutil.Process(os.getpid())
    memory_before = process.memory_info().rss / 1024 / 1024
    
    # å¼ºåˆ¶åŠ è½½åˆ°å†…å­˜
    data = chunk[list(chunk.data_vars.keys())[0]].values
    
    memory_after = process.memory_info().rss / 1024 / 1024
    memory_used = memory_after - memory_before
    
    print(f"Chunkå†…å­˜ä½¿ç”¨: {memory_used:.1f} MB")
    return data

# ç›‘æ§chunkå†…å­˜ä½¿ç”¨
chunk_data = monitor_chunk_memory(smart_chunk)
```

### 3. æ‰¹é‡å¤„ç†å¤šä¸ªchunks
```python
def process_all_chunks(ds, chunk_size=50):
    """æ‰¹é‡å¤„ç†æ‰€æœ‰chunks"""
    results = []
    total_time_steps = ds.dims['time']
    
    for start in range(0, total_time_steps, chunk_size):
        end = min(start + chunk_size, total_time_steps)
        chunk = ds.isel(time=slice(start, end))
        
        # å¤„ç†å½“å‰chunk
        if ds.data_vars:
            first_var = list(ds.data_vars.keys())[0]
            chunk_result = chunk[first_var].mean(dim='time')
            results.append(chunk_result)
            
        print(f"å¤„ç†chunk {start}:{end}")
    
    return results

# å¤„ç†æ‰€æœ‰chunks
all_results = process_all_chunks(ds, chunk_size=50)
```

## ğŸ¯ æœ€ä½³å®è·µæ€»ç»“

1. **æŒ‰éœ€è¯»å–**: åªè¯»å–ä½ éœ€è¦çš„chunk
2. **å†…å­˜æ§åˆ¶**: æ ¹æ®å¯ç”¨å†…å­˜é€‰æ‹©chunkå¤§å°
3. **æ—¶é—´ä¼˜å…ˆ**: é€šå¸¸æŒ‰æ—¶é—´chunkè¯»å–æœ€å®ç”¨
4. **ç›‘æ§å†…å­˜**: å®æ—¶ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ
5. **æ‰¹é‡å¤„ç†**: å¯¹äºå¤§æ–‡ä»¶ï¼Œé€ä¸ªå¤„ç†chunks

è¿™æ ·ä½ å°±å¯ä»¥é«˜æ•ˆåœ°å¤„ç†å¤§å‹NCæ–‡ä»¶ï¼Œé¿å…å†…å­˜æº¢å‡ºï¼ŒåŒæ—¶ä¿æŒåˆ†æçš„çµæ´»æ€§ï¼

```python
# åœ¨ä½ çš„notebookä¸­æ·»åŠ è¿™ä¸ªcell

print("=== æ–¹æ³•1: ç´¢å¼•åˆ‡ç‰‡è¯»å–å•ä¸ªchunk ===")

# å‡è®¾ä½ æƒ³è¯»å–ç¬¬ä¸€ä¸ªchunk
# æ ¹æ®ä¹‹å‰çš„chunksä¿¡æ¯: time(174, 112), y(360, 120), x(360, 120)

# è¯»å–ç¬¬ä¸€ä¸ªå®Œæ•´çš„chunk (174, 360, 360)
chunk1 = ds.isel(
    time=slice(0, 174),      # ç¬¬ä¸€ä¸ªæ—¶é—´å—
    y=slice(0, 360),         # ç¬¬ä¸€ä¸ªYå—  
    x=slice(0, 360)          # ç¬¬ä¸€ä¸ªXå—
)

print(f"ç¬¬ä¸€ä¸ªchunkå½¢çŠ¶: {chunk1[list(ds.data_vars.keys())[0]].shape}")
print(f"ç¬¬ä¸€ä¸ªchunkå¤§å°: {chunk1[list(ds.data_vars.keys())[0]].nbytes / 1024 / 1024:.2f} MB")

# è¯»å–æœ€åä¸€ä¸ªchunk (112, 120, 120)
chunk_last = ds.isel(
    time=slice(174, 286),    # ç¬¬äºŒä¸ªæ—¶é—´å—
    y=slice(360, 480),       # ç¬¬äºŒä¸ªYå—
    x=slice(360, 480)        # ç¬¬äºŒä¸ªXå—
)

print(f"æœ€åä¸€ä¸ªchunkå½¢çŠ¶: {chunk_last[list(ds.data_vars.keys())[0]].shape}")
```

```python
# æ–¹æ³•2: è‡ªåŠ¨è®¡ç®—chunkè¾¹ç•Œ
print("\n=== æ–¹æ³•2: è‡ªåŠ¨è®¡ç®—chunkè¾¹ç•Œ ===")

def get_chunk_boundaries(ds, chunk_index):
    """è·å–æŒ‡å®šchunkçš„è¾¹ç•Œç´¢å¼•"""
    boundaries = {}
    
    for dim, chunks in ds.chunks.items():
        start_idx = 0
        for i, chunk_size in enumerate(chunks):
            if i == chunk_index:
                boundaries[dim] = (start_idx, start_idx + chunk_size)
                break
            start_idx += chunk_size
    
    return boundaries

# è·å–ç¬¬ä¸€ä¸ªchunkçš„è¾¹ç•Œ
chunk_0_boundaries = get_chunk_boundaries(ds, 0)
print(f"ç¬¬ä¸€ä¸ªchunkè¾¹ç•Œ: {chunk_0_boundaries}")

# ä½¿ç”¨è¾¹ç•Œè¯»å–æ•°æ®
if chunk_0_boundaries:
    chunk_data = ds.isel({
        dim: slice(start, end) 
        for dim, (start, end) in chunk_0_boundaries.items()
    })
    print(f"è¯»å–çš„chunkå½¢çŠ¶: {chunk_data[list(ds.data_vars.keys())[0]].shape}")
```

```python
# æ–¹æ³•3: æŒ‰æ—¶é—´chunkè¯»å–ï¼ˆæœ€å¸¸ç”¨ï¼‰
print("\n=== æ–¹æ³•3: æŒ‰æ—¶é—´chunkè¯»å– ===")

# è¯»å–ç¬¬ä¸€ä¸ªæ—¶é—´chunkçš„æ‰€æœ‰ç©ºé—´æ•°æ®
time_chunk_1 = ds.isel(time=slice(0, 174))
print(f"ç¬¬ä¸€ä¸ªæ—¶é—´chunkå½¢çŠ¶: {time_chunk_1[list(ds.data_vars.keys())[0]].shape}")

# è¯»å–ç¬¬äºŒä¸ªæ—¶é—´chunkçš„æ‰€æœ‰ç©ºé—´æ•°æ®  
time_chunk_2 = ds.isel(time=slice(174, 286))
print(f"ç¬¬äºŒä¸ªæ—¶é—´chunkå½¢çŠ¶: {time_chunk_2[list(ds.data_vars.keys())[0]].shape}")

# è¯»å–ç‰¹å®šæ—¶é—´èŒƒå›´
time_subset = ds.isel(time=slice(0, 50))  # å‰50ä¸ªæ—¶é—´æ­¥
print(f"å‰50ä¸ªæ—¶é—´æ­¥å½¢çŠ¶: {time_subset[list(ds.data_vars.keys())[0]].shape}")
```

```python
# æ–¹æ³•4: æŒ‰ç©ºé—´chunkè¯»å–
print("\n=== æ–¹æ³•4: æŒ‰ç©ºé—´chunkè¯»å– ===")

# è¯»å–ç¬¬ä¸€ä¸ªç©ºé—´chunkçš„æ‰€æœ‰æ—¶é—´æ•°æ®
spatial_chunk_1 = ds.isel(y=slice(0, 360), x=slice(0, 360))
print(f"ç¬¬ä¸€ä¸ªç©ºé—´chunkå½¢çŠ¶: {spatial_chunk_1[list(ds.data_vars.keys())[0]].shape}")

# è¯»å–ä¸­å¿ƒåŒºåŸŸ
center_chunk = ds.isel(
    y=slice(120, 360),  # ä¸­å¿ƒYåŒºåŸŸ
    x=slice(120, 360)   # ä¸­å¿ƒXåŒºåŸŸ
)
print(f"ä¸­å¿ƒåŒºåŸŸchunkå½¢çŠ¶: {center_chunk[list(ds.data_vars.keys())[0]].shape}")
```

```python
# æ–¹æ³•5: ä½¿ç”¨Daskçš„chunkè¿­ä»£
print("\n=== æ–¹æ³•5: Dask chunkè¿­ä»£ ===")

def process_chunks_sequentially(ds, max_chunks=3):
    """é€ä¸ªå¤„ç†chunksï¼Œé¿å…å†…å­˜æº¢å‡º"""
    results = []
    
    if ds.data_vars:
        first_var = list(ds.data_vars.keys())[0]
        var = ds[first_var]
        
        # æŒ‰æ—¶é—´chunkå¤„ç†
        time_chunks = ds.chunks['time']
        start_idx = 0
        
        for i, chunk_size in enumerate(time_chunks[:max_chunks]):
            end_idx = start_idx + chunk_size
            
            print(f"å¤„ç†æ—¶é—´chunk {i+1}: {start_idx}:{end_idx}")
            
            # è¯»å–å½“å‰chunk
            current_chunk = ds.isel(time=slice(start_idx, end_idx))
            chunk_data = current_chunk[first_var]
            
            # è®¡ç®—ç»Ÿè®¡é‡
            chunk_mean = chunk_data.mean(dim='time')
            results.append(chunk_mean)
            
            print(f"  chunkå½¢çŠ¶: {chunk_data.shape}")
            print(f"  chunkå‡å€¼: {chunk_mean.mean().values:.2f}")
            
            start_idx = end_idx
    
    return results

# å¤„ç†å‰3ä¸ªæ—¶é—´chunks
chunk_results = process_chunks_sequentially(ds, max_chunks=2)
print(f"å¤„ç†äº† {len(chunk_results)} ä¸ªchunks")
```

```python
# æ–¹æ³•6: æ™ºèƒ½chunké€‰æ‹©
print("\n=== æ–¹æ³•6: æ™ºèƒ½chunké€‰æ‹© ===")

def read_smart_chunk(ds, target_memory_mb=100):
    """æ ¹æ®ç›®æ ‡å†…å­˜å¤§å°æ™ºèƒ½é€‰æ‹©chunk"""
    
    if ds.data_vars:
        first_var = list(ds.data_vars.keys())[0]
        var = ds[first_var]
        
        # è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„å†…å­˜å¤§å°
        time_chunks = ds.chunks['time']
        spatial_size = ds.dims['y'] * ds.dims['x']
        bytes_per_pixel = var.dtype.itemsize
        bytes_per_timestep = spatial_size * bytes_per_pixel
        
        # è®¡ç®—èƒ½å®¹çº³å¤šå°‘ä¸ªæ—¶é—´æ­¥
        max_timesteps = int(target_memory_mb * 1024 * 1024 / bytes_per_timestep)
        
        print(f"ç›®æ ‡å†…å­˜: {target_memory_mb} MB")
        print(f"æ¯ä¸ªæ—¶é—´æ­¥å¤§å°: {bytes_per_timestep / 1024 / 1024:.2f} MB")
        print(f"å»ºè®®æ—¶é—´æ­¥æ•°: {max_timesteps}")
        
        # é€‰æ‹©åˆé€‚çš„æ—¶é—´èŒƒå›´
        actual_timesteps = min(max_timesteps, time_chunks[0])
        smart_chunk = ds.isel(time=slice(0, actual_timesteps))
        
        print(f"å®é™…è¯»å–å½¢çŠ¶: {smart_chunk[first_var].shape}")
        print(f"å®é™…å†…å­˜ä½¿ç”¨: {smart_chunk[first_var].nbytes / 1024 / 1024:.2f} MB")
        
        return smart_chunk

# è¯»å–100MBçš„chunk
smart_chunk = read_smart_chunk(ds, target_memory_mb=100)
```

```python
# æ—¶é—´åºåˆ—åˆ†æ - æŒ‰æ—¶é—´chunkè¯»å–
time_analysis_chunk = ds.isel(time=slice(0, 50))

# ç©ºé—´åˆ†æ - æŒ‰ç©ºé—´chunkè¯»å–  
spatial_analysis_chunk = ds.isel(y=slice(0, 240), x=slice(0, 240))

# ç‰¹å®šåŒºåŸŸåˆ†æ - è‡ªå®šä¹‰åŒºåŸŸ
region_chunk = ds.isel(
    time=slice(0, 100),
    y=slice(100, 300), 
    x=slice(100, 300)
)
```

```python
import psutil
import os

def monitor_chunk_memory(chunk):
    """ç›‘æ§chunkçš„å†…å­˜ä½¿ç”¨"""
    process = psutil.Process(os.getpid())
    memory_before = process.memory_info().rss / 1024 / 1024
    
    # å¼ºåˆ¶åŠ è½½åˆ°å†…å­˜
    data = chunk[list(chunk.data_vars.keys())[0]].values
    
    memory_after = process.memory_info().rss / 1024 / 1024
    memory_used = memory_after - memory_before
    
    print(f"Chunkå†…å­˜ä½¿ç”¨: {memory_used:.1f} MB")
    return data

# ç›‘æ§chunkå†…å­˜ä½¿ç”¨
chunk_data = monitor_chunk_memory(smart_chunk)
```

```python
def process_all_chunks(ds, chunk_size=50):
    """æ‰¹é‡å¤„ç†æ‰€æœ‰chunks"""
    results = []
    total_time_steps = ds.dims['time']
    
    for start in range(0, total_time_steps, chunk_size):
        end = min(start + chunk_size, total_time_steps)
        chunk = ds.isel(time=slice(start, end))
        
        # å¤„ç†å½“å‰chunk
        if ds.data_vars:
            first_var = list(ds.data_vars.keys())[0]
            chunk_result = chunk[first_var].mean(dim='time')
            results.append(chunk_result)
            
        print(f"å¤„ç†chunk {start}:{end}")
    
    return results

# å¤„ç†æ‰€æœ‰chunks
all_results = process_all_chunks(ds, chunk_size=50)
```

